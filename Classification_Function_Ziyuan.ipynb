{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\directed research\\\\data.csv\")\n",
    "data = data.loc[data.english == '0', ['reviewText', 'Function']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preporcessing: tokenize / remove stopwords / lemmatization / categorical features binarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the stopwords list \n",
    "reserved_words = set(['what', 'which', 'but', 'where', 'why', 'how', 'no', 'not', 'don', \"don't\", 'should', 'shouldn', \"shouldn't\"\n",
    "                 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "                 \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "removal_list = list(set(stopwords.words('english')).difference(reserved_words))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the part-of-speech for each word to get an accurate lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    treebank_tag = pos_tag([word])[0][1]\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform nominal variables into binary format\n",
    "def feature_binarizing(row):\n",
    "    if  pd.isnull(row['Function']):\n",
    "        row['otherfunctional'] = 1\n",
    "        return row\n",
    "    if 'featureRequest' in row['Function']:\n",
    "        row['featureRequest'] = 1\n",
    "    if 'featureRemoval' in row['Function']:\n",
    "        row['featureRemoval'] = 1\n",
    "    if 'functerr' in row['Function']:\n",
    "        row['functerr'] = 1\n",
    "    if 'otherfunctional' in row['Function']:\n",
    "        row['otherfunctional'] = 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews and lemmatize each token, remove any non-alphanumeric characters\n",
    "def tokenize_lemmatize(review):\n",
    "    tokens = word_tokenize(review)\n",
    "    tokens = [t.lower() for t in tokens if t.isalnum()]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in tokens]\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['featureRequest'] = 0\n",
    "data['featureRemoval'] = 0\n",
    "data['functerr'] = 0\n",
    "data['otherfunctional'] = 0\n",
    "data = data.apply(feature_binarizing, axis=1)\n",
    "data['reviewText'] = data.apply(lambda x: tokenize_lemmatize(x.reviewText), axis = 1)\n",
    "data = data[data.reviewText != 0]\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features representation: tf-idf * word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tf-idf matrix of our corpus and save it into variable 'matrix', which is used as the features in the model trainging\n",
    "tfidf = TfidfVectorizer(stop_words=removal_list, analyzer=lambda x: x, max_df=0.9, sublinear_tf=True)\n",
    "matrix = tfidf.fit_transform(list(data.reviewText)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vectors matrix and save it into variable 'w2v', the dimension of the word vector is 300\n",
    "model = Word2Vec(data.reviewText, size=300, workers=4, min_count=1)\n",
    "w2v = dict(zip(model.wv.index2entity, model.wv.vectors))\n",
    "\n",
    "# Get the sentence vectors: \n",
    "# Each sentence vector is equivalent to the sum of the vectors of the words in this sentence, which is weighted by tf-idf matrix\n",
    "# The result is saved into variable 's2v', which is used as the features in the model training\n",
    "s2v = np.array([np.array([w2v[word] * matrix[index, tfidf.vocabulary_[word]] for word in sentence]).mean(axis=0) \n",
    "                   for index, sentence in data.reviewText.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Naive Bayes / SVM(RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "# The performance of Naive Bayes is worss than svm\n",
    "#mnb = MultinomialNB()\n",
    "#bnb = BernoulliNB()\n",
    "\n",
    "# SVM(Radial Basis Function (RBF) kernel)\n",
    "# Use function GridSearchCV() to choose the best parameters\n",
    "# Best parameters: C=3 and gamma=1 for tfidf matrix, C=5700 and gamma=100 for s2v\n",
    "'''\n",
    "parameters = {'C': [5500, 5700, 5900]}\n",
    "clf = GridSearchCV(svc, parameters, scoring='f1', cv=3)\n",
    "clf.fit(s2v, data.featureRequest)\n",
    "clf.best_params_\n",
    "'''\n",
    "svc = svm.SVC(kernel='rbf', class_weight= 'balanced', C=5700, gamma=100, decision_function_shape=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureRequest + s2v\n",
      "precision: 0.2338730141328776\n",
      "recall: 0.7636507936507937\n",
      "f1: 0.3579239529742898\n"
     ]
    }
   ],
   "source": [
    "# 10 fold cross validation\n",
    "skf = StratifiedKFold(10, shuffle = True).get_n_splits()\n",
    "scoring = ['precision', 'recall', 'f1']\n",
    "\n",
    "# Test the performance, \n",
    "# Two types of features: tfidf matrix vs sentence vectors\n",
    "# Two tragets: funtion error vs feature request\n",
    "\n",
    "print('functerr + tfidf matrix')\n",
    "scores = cross_validate(svc, matrix, data.functerr, scoring=scoring, cv=skf, return_train_score=False)\n",
    "print('precision:', scores['test_precision'].mean())\n",
    "print('recall:', scores['test_recall'].mean())\n",
    "print('f1:', scores['test_f1'].mean())\n",
    "print('--------------------------')\n",
    "\n",
    "print('featureRequest + tfidf matrix')\n",
    "scores = cross_validate(svc, matrix, data.featureRequest, scoring=scoring, cv=skf, return_train_score=False)\n",
    "print('precision:', scores['test_precision'].mean())\n",
    "print('recall:', scores['test_recall'].mean())\n",
    "print('f1:', scores['test_f1'].mean())\n",
    "print('--------------------------')\n",
    "\n",
    "print('functerr + s2v')\n",
    "scores = cross_validate(svc, s2v, data.functerr, scoring=scoring, cv=skf, return_train_score=False)\n",
    "print('precision:', scores['test_precision'].mean())\n",
    "print('recall:', scores['test_recall'].mean())\n",
    "print('f1:', scores['test_f1'].mean())\n",
    "print('--------------------------')\n",
    "\n",
    "print('featureRequest + s2v')\n",
    "scores = cross_validate(svc, s2v, data.featureRequest, scoring=scoring, cv=skf, return_train_score=False)\n",
    "print('precision:', scores['test_precision'].mean())\n",
    "print('recall:', scores['test_recall'].mean())\n",
    "print('f1:', scores['test_f1'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
